{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00105b8b-5b5c-49b9-9557-2969dc2dc974",
   "metadata": {},
   "source": [
    "# Convert Data to C4 Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caaffad6-7e8a-4877-bdc0-a3b6cd193531",
   "metadata": {},
   "source": [
    "## Check dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eaf25324-7e7d-45c3-8a12-7cc8e788d302",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import hashlib\n",
    "from gensim.models import KeyedVectors\n",
    "import datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eab1c75-03e1-4b49-8b00-2ad5ef49e84a",
   "metadata": {},
   "source": [
    "Following files are generated by [parsec/20.32](https://github.com/seantyh/parsec/blob/main/etc/20.32-tencent-StandfordNLP-small.ipynb):\n",
    "```\n",
    "tencent_nn_embeddings-s.txt b237d2\n",
    "tencent_vocab_nn_compounds_small a29857\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d60ff95e-19a1-441c-9a5f-eaf7ca149a7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tencent_nn_embeddings-s.txt.gz b237d2\n",
      "tencent_vocab_nn_compounds_small.txt a29857\n"
     ]
    }
   ],
   "source": [
    "vec_path = Path(\"../data/tencent_nn_embeddings-s.txt.gz\")\n",
    "nn_path = Path(\"../data/tencent_vocab_nn_compounds_small.txt\")\n",
    "for path_x in (vec_path, nn_path):\n",
    "    h = hashlib.sha1()\n",
    "    h.update(path_x.read_bytes())\n",
    "    print(\"{} {}\".format(path_x.name, h.digest().hex()[:6]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c1cfab-ec56-44bd-ad24-e7aa3454127c",
   "metadata": {},
   "source": [
    "## Convert to Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12491925-0a94-48f4-a443-baca5ae4f400",
   "metadata": {},
   "outputs": [],
   "source": [
    "kv = KeyedVectors.load_word2vec_format(vec_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "889790e2-5eb8-44f9-bb91-4d002dc32b89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({1: 10818, 2: 27901, 4: 192882})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "Counter(len(x) for x in kv.index_to_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c1229f0-4e60-4e90-af10-463425db8ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "compounds = pd.read_csv(nn_path, header=None, \n",
    "                        names=\"nn c1 c2\".split())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb051d9-f79c-470a-be5e-9d35c0f00e86",
   "metadata": {},
   "source": [
    "## Build C4 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "57da0c6a-b408-4bf6-860a-3c9dd9ce9c5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.00690007209777832,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 22,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 192882,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2260420d8e64a7dbfb4ab6a2fec4d34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/192882 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "comp_mat = []\n",
    "c1_mat = []\n",
    "c2_mat = []\n",
    "for comp_x in tqdm(compounds.itertuples(), total=compounds.shape[0]):\n",
    "    cc_x = comp_x.nn\n",
    "    c1_x = comp_x.c1\n",
    "    c2_x = comp_x.c2\n",
    "    cc_vec = kv.get_vector(cc_x, norm=False)\n",
    "    c1_vec = kv.get_vector(c1_x, norm=False)\n",
    "    c2_vec = kv.get_vector(c2_x, norm=False)\n",
    "    comp_mat.append(cc_vec)\n",
    "    c1_mat.append(c1_vec)\n",
    "    c2_mat.append(c2_vec)\n",
    "comp_mat = np.vstack(comp_mat)\n",
    "c1_mat = np.vstack(c1_mat)\n",
    "c2_mat = np.vstack(c2_mat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2afcdce0-a0b4-4d93-afbf-1483cf571ae3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(192882, 200)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comp_mat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "54303539-aa6a-40f0-b345-84035fa4aa17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train/val/test:  154305 19288 19289\n"
     ]
    }
   ],
   "source": [
    "N = comp_mat.shape[0]\n",
    "split_idxs = np.arange(N)\n",
    "rng = np.random.RandomState(123)\n",
    "rng.shuffle(split_idxs)\n",
    "train_idxs = split_idxs[:int(N*.8)]\n",
    "val_idxs = split_idxs[int(N*.8):int(N*.9)]\n",
    "test_idxs = split_idxs[int(N*.9):]\n",
    "assert len(set(train_idxs) & set(val_idxs)) == 0\n",
    "assert len(set(train_idxs) & set(test_idxs)) == 0\n",
    "assert len(set(val_idxs) & set(test_idxs)) == 0\n",
    "print(\"train/val/test: \", len(train_idxs), len(val_idxs), len(test_idxs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a30f2e4c-580b-4ce2-aacb-9b68937335b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "consts = np.concatenate([c1_mat, c2_mat], axis=1)\n",
    "train_comps = comp_mat[train_idxs, :]\n",
    "val_comps = comp_mat[val_idxs, :]\n",
    "test_comps = comp_mat[test_idxs, :]\n",
    "train_consts = consts[train_idxs, :]\n",
    "val_consts = consts[val_idxs, :]\n",
    "test_consts = consts[test_idxs, :]\n",
    "train_comps_text = compounds.nn[train_idxs]\n",
    "val_comps_text = compounds.nn[val_idxs]\n",
    "test_comps_text = compounds.nn[test_idxs]\n",
    "train_c1_text = compounds.c1[train_idxs]\n",
    "val_c1_text = compounds.c1[val_idxs]\n",
    "test_c1_text = compounds.c1[test_idxs]\n",
    "train_c2_text = compounds.c2[train_idxs]\n",
    "val_c2_text = compounds.c2[val_idxs]\n",
    "test_c2_text = compounds.c2[test_idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "82fc9f61-3432-477c-a36e-d0f91aa6c8a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "comps train/val/test:  (154305, 200) (19288, 200) (19289, 200)\n",
      "consts train/val/test:  (154305, 400) (19288, 400) (19289, 400)\n",
      "comps_text train/val/test:  154305 19288 19289\n",
      "c1_text train/val/test:  154305 19288 19289\n",
      "c2_text train/val/test:  154305 19288 19289\n"
     ]
    }
   ],
   "source": [
    "print(\"comps train/val/test: \", train_comps.shape, val_comps.shape, test_comps.shape)\n",
    "print(\"consts train/val/test: \", train_consts.shape, val_consts.shape, test_consts.shape)\n",
    "print(\"comps_text train/val/test: \", len(train_comps_text), len(val_comps_text), len(test_comps_text))\n",
    "print(\"c1_text train/val/test: \", len(train_c1_text), len(val_c1_text), len(test_c1_text))\n",
    "print(\"c2_text train/val/test: \", len(train_c2_text), len(val_c2_text), len(test_c2_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca34c94f-908e-4780-b3c0-255bf29248ad",
   "metadata": {},
   "source": [
    "### Build dataset dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "069ed59f-7702-49cf-b8c9-b39fe2858a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "c4_dataset = {\n",
    "    \"train\": {\n",
    "        \"comps\": train_comps,\n",
    "        \"consts\": train_consts,\n",
    "        \"comps_text\": train_comps_text,\n",
    "        \"c1_text\": train_c1_text,\n",
    "        \"c2_text\": train_c2_text,\n",
    "    },\n",
    "    \"val\": {\n",
    "        \"comps\": val_comps,\n",
    "        \"consts\": val_consts,\n",
    "        \"comps_text\": val_comps_text,\n",
    "        \"c1_text\": val_c1_text,\n",
    "        \"c2_text\": val_c2_text,\n",
    "    },\n",
    "    \"test\": {\n",
    "        \"comps\": test_comps,\n",
    "        \"consts\": test_consts,\n",
    "        \"comps_text\": test_comps_text,\n",
    "        \"c1_text\": test_c1_text,\n",
    "        \"c2_text\": test_c2_text,\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff5d6554-fe53-45e4-a723-89cc0abab5ea",
   "metadata": {},
   "source": [
    "## Write to file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f466c65-6002-4270-9fa8-e86976ef8937",
   "metadata": {},
   "source": [
    "```\n",
    "tencent-compound-c4.pkl 0eb1e3\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7b3e4bbf-34b4-4f02-95b8-d719ef7666a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tencent-compound-c4.pkl 0eb1e3\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "c4_path = \"../data/tencent-compound-c4.pkl\"\n",
    "with open(c4_path, \"wb\") as fout:\n",
    "    pickle.dump(c4_dataset, fout)\n",
    "h = hashlib.sha1()\n",
    "h.update(Path(c4_path).read_bytes())\n",
    "print(Path(c4_path).name, h.digest().hex()[:6])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
